\begin{abstract}
We use Hanabi—a long-horizon cooperative, partially observable multi-agent game—as a controlled setting to probe several open questions about reinforcement learning (RL) for LLM agents: whether RL induces capability beyond greedy selection, whether it improves exploration in ways that transfer to adjacent tasks, how robust learned behaviors are across prompting scaffolds, and whether learned skills can be composed via model merging. We train open models with GRPO-style RL using two complementary supervision signals: (i) belief-state tracking, where the agent must update its internal state given a prior state and intervening actions, and (ii) action evaluation, where an LLM judge provides dense move ratings for candidate actions.

RL on belief-state tracking yields large gains at high sampling budgets: on a held-out belief-update validation set, accuracy at Pass@1024 improves from 11/62 for the base model to 58/62 after RL. These improvements transfer across tasks and data sources: Hanabi-trained state tracking improves a separate belief-update evaluation to 41/62 at Pass@1024, and a tracking-only model improves a Hanabi state-tracking validation score from 0.275 to 0.374. We further find that the learned belief-tracking skill is modular and portable: merging a belief-tracking LoRA into a “thinking” base model improves accuracy from 31/62 to 51/62, despite training the adapter on an instruct variant. Finally, scaffold variations disentangle policy improvement from knowledge injection: RL improves settings whose prompts already include Hanabi rules, but does not reliably transfer to a scaffold that omits game knowledge. Overall, our results suggest RL can produce transferable, composable skill adapters that improve exploration and long-horizon state tracking, while highlighting concrete limits of RL-based knowledge injection across scaffolds.
\end{abstract}