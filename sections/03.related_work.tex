\section{Related Work}
\label{sec:related_work}

\paragraph{RL post-training for reasoning and agents.}
Modern LLM post-training commonly combines supervised fine-tuning with RL-based optimization (e.g., RLHF/RLAIF variants). The InstructGPT line popularized the SFT$\rightarrow$RL pipeline for instruction following \citep{ouyang2022instructgpt}. More recently, RL with verifiable rewards has produced large improvements in domains with automatic checking, particularly math and code, motivating scalable on-policy algorithms such as GRPO-style methods \citep{deepseekr1_2025,grpo_style_2025}. These successes renew a foundational question: does RL create new capability, or does it primarily reweight existing behaviors?

\paragraph{Pass@K, exploration, and the capability--selection debate.}
Recent work argues that RLVR can disproportionately improve Pass@1 while leaving large-$K$ performance unchanged or worse, consistent with a conservative policy/support preservation story \citep{yue2025does,wu2025limitrlvr}. In parallel, Pass@K-oriented objectives have been proposed to explicitly reward ``at least one succeeds'' behavior across a batch of samples, improving exploration and preventing early collapse into conservative modes \citep{walder2025pkpo}. Our results contribute a multi-agent POMDP case where RL produces large-$K$ gains that transfer to adjacent tasks, helping map the boundary between re-ranking and exploration.

\paragraph{Model merging and mode connectivity.}
Model merging is motivated by weight-space ensembling and observations about connectivity in neural loss landscapes. Weight averaging methods such as stochastic weight averaging and later ``model soups'' show that carefully averaged weights can yield robust improvements without increasing inference cost \citep{izmailov2018swa,wortsman2022model}. In the LLM era, task-vector and merging methods aim to reduce interference and improve multi-skill composition, including task arithmetic/task vectors and sign-based methods such as TIES \citep{ilharco2023task,yao2024ties}. Our work studies RL-trained agent skills (belief-state tracking and move evaluation) and shows they can compose as portable modules, including transfer across base variants (instruct$\rightarrow$thinking) and balanced competence when merging specialists.

\paragraph{Parameter-efficient modules and compositionality.}
LoRA enables parameter-efficient specialization by learning low-rank updates while freezing the base model \citep{lora_2021}. Prior work explores composing or selecting among learned modules, including module libraries such as LoraHub \citep{lorahub_2023}. We show that RL-trained belief-state tracking behaves as a composable module in an agentic, long-horizon POMDP setting and transfers across base ``modes'' (instruct vs.\ thinking), strengthening the case for modular post-training in agent pipelines.

\paragraph{LLMs for Hanabi and cooperative play.}
Hanabi has been established as a benchmark for cooperative AI and multi-agent reasoning under partial information \citep{bard2019hanabi}. Recent work explores LLM-based coordination and Hanabi-like cooperative decision making with structured prompting, theory-of-mind style reasoning, hybrid pipelines, or specialized training \citep{agashe2023llmcoordination,sudhakar2024generalist,hu2023instructedrl}. Our work uses Hanabi primarily as a controlled testbed to interrogate RL-for-LLMs questions---particularly Pass@K behavior, adjacent-task transfer via belief-state tracking, scaffold robustness, and modular composition through merging---rather than only optimizing end-to-end score.
